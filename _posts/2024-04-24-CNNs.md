# Convolutional Neural Networks with Fastai

**Date:** 2024-04-24

**Categories:** AI, Fastai

---

## Introduction

As part of my journey in ELEC4630, I have dived deep into the Fastai course. There I have refreshed my memory on convolutional neural networks (CNNS). CNNS are the bread and butter of artificial intelligence and have revolutionised many fields. In this blog post, I will share my insight and document what I learnt from the [fast.ai course blog on CNNs](https://course.fast.ai/Lessons/lesson8.html).


## What is a Convolutional Neural Network?

![cnn](https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max)
Image from [prismic](https://images.prismic.io/encord/11b9026c-edc4-4d23-b6f3-09bd0ede3e28_image+%2835%29+2.jpg?auto=compress%2Cformat&fit=max)

CNNs as the name suggests are a type of neural network which convolutes a different kernel upon each layer. What does this mean? Well, CNNs are effective at feature extraction. An example would be for image detection. By applying filters to the input images in each of the layers, they can identify features such as edges and shapes, which are then used to understand larger patterns (like objects) in deeper layers.

## Key Components of CNNs

### 1. Convolutional Layer

The core building block of a CNN is the convolutional layer. This layer uses kernels to convolute an operation across the data. As the filter slides over the image, it computes the dot product between the filter and the input, producing a feature map. These features can start as edges, and then gradually morph into shapes and  eventually specific objects in the deeper layers of a CNN.

**Functionality:**
- **Feature Extraction**: Each filter extracts specific features, such as edges, textures and shapes.

### 2. Activation Layer

After a convolution layer, we apply an activation function. The most common activation function in CNNs and the one I have used the most in the past is the Rectified Linear Unit (ReLU). This layer introduces non-linearity to the system. I like to think of this as, how can you construct an effective linear system to solve a non-linear problem? The answer is you can't. So, that's where ReLU and other non-linear activation functions come into play.

**Functionality:**
- **Non-linear**: Lets the network learn non-linear relationships in the data.

### 3. Pooling Layer

This layer reduces the spatial dimensions (i.e., width and height) of the input data for the next convolutional layer. This serves two purposes. One, it decreases the computational requirements to process the data by decreasing the dimension of it. Two, it forces the future convolution layers to extract patterns that are invariant in size and more generalized. This results in a more robust model that is less likely to overfit.

**Types of Pooling Layers:**
- **Max pooling**: Takes the maximum value of the data covered by the kernel.
- **Average pooling**: Takes the average of the values covered by the kernel.

### 4. Fully Connected Layer

Fully connected layers (FCC) are one of the final steps in a CNN model. The role of FCCs is to classify the data based on the features extracted by the convolutional layers. These are often followed by a softmax layer to produce a probability distribution for the likelihood of the input image being a certain class.

### 5. Dropout Layer

These are used to prevent overfitting. The dropout layer randomly sets a percentage of the input units to 0 during training. This forces the model to learn a more robust and generalized method which can deal with more noisy data.

### 6. Batch Normalization Layer

Batch normalization as the name implies normalizes the output of the previous layers to improve the stability of the training. Here's how it's incorporated in PyTorch:

```python
model = nn.Sequential(
    nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),
    nn.BatchNorm2d(8),
    nn.ReLU(),
    nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),
    nn.BatchNorm2d(16),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d(1),
    nn.Flatten(),
    nn.Linear(16, 10)
)
```

## Building a Simple CNN

The fast.ai course shows how to construct a simple CNN model. Hereâ€™s a snippet showing how to define a basic CNN using PyTorch's `Sequential`:

```python
import torch.nn as nn

simple_cnn = nn.Sequential(
    nn.Conv2d(1, 4, kernel_size=3, stride=2, padding=1), # layer 1
    nn.ReLU(),
    nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1), # layer 2
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(8*7*7, 10)
)
```

## Training a model
Fastai comes with loads of prebuilt models. The code below is of cnn using resnet18. resnets are a type of CNN which incorporates a residual pattern between shallow and deeper layers. This helps construct deeper CNNs, as without the "memory" feature deep CNNs become "forgetful" in deeper layers. Here you can use the inbuilt fastai methods to quickly train one cycle of the CNN and to see its loss function.


```python
from fastai.vision.all import *

path = untar_data(URLs.MNIST_SAMPLE)
dls = ImageDataLoaders.from_folder(path)

learn = cnn_learner(dls, resnet18, metrics=accuracy)
learn.fit_one_cycle(1, 0.001)
```


## Conclusion
CNNs are a super useful neural network architecture which can be used for a wide array of tasks such as image detection. If you are interested in them I recommend diving deep and learning more from the fast.ai blog. There you can learn more about them and cool extensions to a basic CNN model which can unlock crazy improvements. I recommend looking at resnets with their residual layer pattern, which allows deeper layers to still communicate with shallower ones.

After going through the jupyternote book and the course I have familiarised myself with many topics regarding CNNs. I believe that by writing this blog I have reinforced my learning as it forced me to articulate my understanding. This process has really semented my understanding and I look forward to continue writing in the future.
